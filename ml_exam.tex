\documentclass[11pt]{article}

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

% For figures and images
\usepackage{graphicx}

% For better formatting
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\setlist[itemize]{itemsep=0pt, parsep=0pt}

% For adding links
\usepackage{hyperref}

\title{Machine Learning Exam Notes}
\author{Tomáš Jelínek}
\date{\today}

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Disclaimer}
These notes are based on the lecture slides from NPFL129 course: \href{https://ufal.mff.cuni.cz/courses/npfl129/2324-winter#home}{Introduction to Machine Learning with Python} in winter semester 2023/24 which is under CC-BY-SA-4.0 license. The notes are not guaranteed to be correct and are not a substitute for the lecture. They are intended to be used as a study aid and should not be used as the only source of information for the exam.
Artificial Intelligence such as GPT-4, GitHub Copilot and possibly others were used in the proccess of writing this document.

% Start writing your notes here
\part{Lecture 1}
\section{Q1.1 Train/Test Data Separation and Generalization}

\textbf{Why Separate Train and Test Data:}
\begin{itemize}
    \item To evaluate the performance of a machine learning model reliably.
    \item Training data is used to fit the model, while test data assesses its performance on unseen data.
    \item Prevents overfitting, ensuring the model generalizes well to new, unseen data.
\end{itemize}

\textbf{Generalization:}
\begin{itemize}
    \item The ability of a model to perform well on new, unseen data.
    \item Indicates how well the model learns the underlying patterns, not just memorizing the training data.
\end{itemize}

\textbf{Relation to Underfitting and Overfitting:}
\begin{itemize}
    \item \textbf{Underfitting:} Model is too simple, fails to capture underlying patterns in data, leading to poor performance on both training and test data.
    \item \textbf{Overfitting:} Model is too complex, captures noise along with patterns in the training data, leading to poor generalization on test data.
\end{itemize}



\section{Q1.2 Define prediction function of a linear regression model and write down L2-regularized mean squared error loss}

\textbf{Prediction Function:}
Given an input vector \( x \in \mathbb{R}^D \), the prediction function \( f \) for linear regression is defined as:
\[
f(x; w, b) = w^T x + b
\]
where \( w \) is the weight vector, \( b \) is the bias term, and \( T \) denotes the transpose of \( w \).

\textbf{\( L_2 \)-Regularized Mean Squared Error Loss:}
The \( L_2 \)-regularized mean squared error loss (also known as Ridge Regression) for a dataset with \( N \) samples is defined as:
\[
L(w, b) = \frac{1}{2N} \sum_{i=1}^{N} (f(x_i; w) - t_i)^2 + \lambda \| w \|^2
\]
where \( t_i \) is the true target value for the \( i \)-th sample, \( \lambda \) is the regularization parameter, and \( \| w \|^2 \) denotes the \( L_2 \) norm of the weight vector, which is the sum of the squares of its components.


\section{Q1.3 Starting from unregularized sum of squares error of a linear regression model, show how the explicit solution can be obtained, assuming \(X^TX\) is regular}

In order to find a minimum of \( \frac{1}{2} \sum_{i=1}^{N} (x_i^T w - t_i)^2 \), we can inspect values where the derivative of the error function is zero, with respect to all weights \( w_j \).

\[
\frac{\partial}{\partial w_j} \frac{1}{2} \sum_{i=1}^{N} (x_i^T w - t_i)^2 = \frac{1}{2} \sum_{i=1}^{N} 2(x_i^T w - t_i)x_{ij} = \sum_{i=1}^{N} x_{ij}(x_i^T w - t_i)
\]

Therefore, we want for all \( j \) that \( \sum_{i=1}^{N} x_{ij}(x_i^T w - t_i) = 0 \). We can rewrite the explicit sum into \( X_{*,j}^T (Xw - t) = 0 \), then write the equations for all \( j \) together using matrix notation as \( X^T(Xw - t) = 0 \), and finally, rewrite to

\[
X^TXw = X^Tt.
\]

The matrix \( X^TX \) is of size \( D \times D \). If it is regular, we can compute its inverse and therefore

\[
w = (X^TX)^{-1}X^Tt.
\]

\part{Lecture 2}
\section{Q2.1 Describe standard gradient descent and compare it to stochastic (i.e., online) gradient descent and minibatch stochastic gradient descent}

\textbf{Standard gradient descent}, also known as batch gradient descent, computes the gradient of the cost function with respect to the parameters (\( w \)) for the entire training dataset:
\[
w \leftarrow w - \alpha \nabla_w E(w)
\]
where \( \alpha \) is the learning rate.

\textbf{Stochastic Gradient Descent (SGD)}, or online gradient descent, on the other hand, updates the parameters for each training example:
\[
\nabla_w E(w) \approx \nabla_w L(y(x_i; w), t_i)
\]
This method is noisier but can converge faster for large datasets.

\textbf{Minibatch SGD} is a compromise between the two, updating the parameters for a small subset of the training data:
\[
\nabla_w E(w) \approx \frac{1}{B} \sum_{i=1}^{B} \nabla_w L(y(x_i; w), t_i)
\]
This approach aims to balance the computational efficiency of standard gradient descent with the faster convergence of SGD.

\section{Q2.2 Write an \( L_2 \)-regularized minibatch SGD algorithm for training a linear regression model, including the explicit formulas of the loss function and its gradient}
The loss function for \( L_2 \)-regularized linear regression is given by:
\[
E(w) = \frac{1}{2} \mathbb{E}_{(x,t)\sim p_{\text{data}}} [(x^T w - t)^2] + \frac{\lambda}{2} \|w\|^2
\]
where \( w \) are the weights, \( x \) is the input, \( t \) is the target, and \( \lambda \) is the regularization parameter.

The gradient of the loss function with respect to the weights is:
\[
\nabla_w E(w) \approx \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} ((x_i^T w - t_i)x_i) + \lambda w
\]
where \( \mathcal{B} \) is a minibatch of examples.

\begin{algorithm}
\subsection*{Pseudocode of the minibatch SGD algorithm}
\begin{algorithmic}[1]
\Require Dataset \( \{X \in \mathbb{R}^{N \times D}, t \in \mathbb{R}^N\} \), learning rate \( \alpha \in \mathbb{R}_+ \), \( L_2 \) strength \( \lambda \in \mathbb{R} \)
\Ensure Weights \( w \in \mathbb{R}^D \) minimizing the regularized MSE of a linear regression model.
\State Initialize \( w \) randomly
\Repeat
    \State Sample a minibatch \( \mathcal{B} \) of examples with indices \( \mathcal{B} \)
    \State Compute gradient \( g \) according to \( \nabla_w E(w) \) using \( \mathcal{B} \)
    \State Update \( w \): \( w \leftarrow w - \alpha \cdot g \)
\Until{convergence or maximum number of iterations is reached}
\end{algorithmic}
\end{algorithm}


\section{Q2.3 Does the SGD algorithm for linear regression always find the best solution on the training data? If yes, explain under what conditions it happens, if not explain why it is not guaranteed to converge}

Stochastic Gradient Descent (SGD) for linear regression does not always guarantee finding the best solution on the training data. It converges to the global optimum if the following conditions are met:

\begin{itemize}
    \item The loss function is convex and continuous.
    \item The learning rate \( \alpha_i \) meets the Robbins-Monro conditions, which are:
    \begin{itemize}
        \item \( \alpha_i > 0 \)
        \item \( \sum_{i=1}^{\infty} \alpha_i = \infty \)
        \item \( \sum_{i=1}^{\infty} \alpha_i^2 < \infty \)
    \end{itemize}
    \item The third condition ensures that \( \alpha_i \rightarrow 0 \) as \( i \rightarrow \infty \).
\end{itemize}

When these conditions are satisfied, SGD converges to the unique optimum of convex problems. However, for non-convex loss functions, SGD is not guaranteed to find the global minimum; it may converge to a local minimum instead. The noise in the gradient estimation due to the stochastic nature of the algorithm can also affect convergence. Thus, while SGD can perform well in practice, especially for large datasets, it doesn't always find the best solution due to these factors.

\section{Q2.4 After training a model with SGD, you ended up with a low training error and a high test error. Using the learning curves, explain what might have happened and what steps you might take to prevent this from happening}

The learning curves might indicate that while the training loss decreases over time, the test loss decreases initially but then starts to increase. This scenario suggests that the \textbf{model is overfitting} to the training data. Overfitting occurs when a model learns the training data too well, including noise and details that do not generalize to unseen data. Consequently, the model performs well on the training data but poorly on the test data.

To prevent overfitting, you can take the following steps:
\begin{enumerate}
    \item Use regularization techniques such as \( L_1 \) (LASSO) or \( L_2 \) (Ridge) to penalize large weights in the model.
    \item Implement early stopping based on validation performance to halt training before overfitting occurs.
    \item Increase the size of the training set if possible, to provide the model with more generalizable examples.
    \item Simplify the model by reducing its complexity to prevent it from capturing noise in the data. (Make less features, use less layers, etc.)
\end{enumerate}

These methods can help in guiding the model to generalize better to unseen data and thus improve its test performance.

Another reason might be that the model \textbf{failed to converge}. In this case, you can try to increase the number of iterations or decrease the learning rate to improve convergence.

\section*{Q2.5 You were provided with a fixed training set and a fixed test set and you are supposed to report model performance on that test set. You need to decide what hyperparameters to use. How will you proceed and why?}

To determine the best hyperparameters for a model given a fixed training and test set, the following procedure should be employed:

\begin{enumerate}
    \item \textbf{Split the training set:} Divide the training set into a smaller training set and a validation set.
    \item \textbf{Hyperparameter tuning:} Use the smaller training set to train different models with various hyperparameter configurations. (Grid Search, Random Search, Hyperband, SMAC, etc.)
    \item \textbf{Validation:} Evaluate the performance of each model on the validation set.
    \item \textbf{Selection:} Choose the hyperparameters that yield the best performance on the validation set.
    \item \textbf{Final Model:} Train a new model on the full training set using the selected hyperparameters.
    \item \textbf{Testing:} Report the model's performance on the fixed test set.
\end{enumerate}

This procedure is crucial because it helps to estimate the model's performance on unseen data and prevents overfitting to the training set. The validation set acts as a proxy for the test set, allowing for an unbiased evaluation of hyperparameter choices.

\section*{Q2.6 What method can be used for normalizing feature values? Explain why it is useful}

Feature normalization can be achieved through methods such as Min-Max normalization and Z-score standardization. These methods are useful for several reasons:

\begin{itemize}
    \item \textbf{Min-Max Normalization:} Scales the features to a fixed range, typically [0, 1]. It is given by the formula:
    \[
    x'_{i,j} = \frac{x_{i,j} - \min_k x_{k,j}}{\max_k x_{k,j} - \min_k x_{k,j}}
    \]
    This method is beneficial when we need to bound our features within a specific scale without distorting differences in the ranges of values.
    
    \item \textbf{Z-score Standardization:} Transforms the features to have a mean of zero and a standard deviation of one. The formula is:
    \[
    x'_{i,j} = \frac{x_{i,j} - \bar{x}_j}{\sigma_j}
    \]
    This is particularly useful in optimization algorithms that require features on a comparable scale for efficient learning.
\end{itemize}

Additionally, techniques similar to PCA, such as Principal Component Analysis itself, can be used for feature scaling and reduction. PCA transforms the data into a new coordinate system, reducing dimensionality and potentially improving model performance by removing noise and redundancy in the data.


\part{Lecture 3}

\section{Q3.1 Define binary classification, write down the perceptron algorithm and show how a prediction is made for a given example}

Binary classification is the task of classifying the elements of a given set into two groups based on a classification rule. In binary classification, the output variable can take only two values, typically denoted as 0 and 1, or -1 and 1 in some contexts.

The perceptron algorithm is a binary classifier that linearly separates these two classes. The algorithm iteratively adjusts the weights based on the training data. Given a set of features \( x \) and a target \( t \), the perceptron rule updates the weights \( w \) as follows:

\begin{algorithmic}
\If {\( t_i (x_{i}^T w) \leq 0 \)}
    \State \( w \gets w + t_i x_i \)
\EndIf
\end{algorithmic}

To make a prediction \( \hat{y} \) for a new example with feature vector \( x \), the perceptron uses the sign of the dot product between the features and weights:

\[
\hat{y} = \text{sign}(x^T w)
\]

where \( \text{sign} \) is an activation function that maps positive values to +1 and non-positive values to -1.

\subsection*{Prediction Example}
Given a new input \( x \) and trained weights \( w \), the perceptron prediction is computed as:
\[
\hat{y} = \text{sign}(x^T w + b)
\]
where \( b \) is the bias term of the perceptron. If \( \hat{y} \) is positive, the input is classified into one class, and if it is negative, it is classified into the other class.


\section{Q3.2 Define entropy, cross-entropy, Kullback-Leibler divergence, and prove the Gibbs inequality}

Entropy \( H(P) \) for a discrete random variable with probability distribution \( P \) is defined as:
\[
H(P) = - \sum_x P(x) \log P(x)
\]
It measures the expected level of 'surprise' or uncertainty inherent in the variable's possible outcomes.

Cross-entropy \( H(P, Q) \) between two discrete probability distributions \( P \) and \( Q \) is defined as:
\[
H(P, Q) = - \sum_x P(x) \log Q(x)
\]
It measures the expected number of bits (if the log is in base 2) required to identify an event from a set of possibilities if a wrong distribution \( Q \) is used instead of the true distribution \( P \).

Kullback-Leibler divergence \( D_{KL}(P||Q) \) from \( Q \) to \( P \) is defined as:
\[
D_{KL}(P||Q) = H(P, Q) - H(P) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
\]
It measures how one probability distribution diverges from a second, expected probability distribution.

\textbf{Proof of Gibbs Inequality:}
We want to prove that \( D_{KL}(P||Q) \geq 0 \), with equality if and only if \( P = Q \).

Using the log sum inequality \( \log \frac{a}{b} \leq \frac{a}{b} - 1 \) with equality only if \( a = b \), we have:

\begin{align*}
H(P) - H(P,Q) &= \sum_x P(x) \log \frac{Q(x)}{P(x)} \\
&\leq \sum_x P(x) \left( \frac{Q(x)}{P(x)} - 1 \right) \\
&= \sum_x Q(x) - \sum_x P(x) \\
&= 0
\end{align*}

since \( \sum_x P(x) = 1 \) and \( \sum_x Q(x) = 1 \). The inequality is strict unless \( P(x) = Q(x) \) for all \( x \), which proves Gibbs Inequality.


\section{Q3.3 Explain the notion of likelihood in maximum likelihood estimation}

Likelihood in the context of maximum likelihood estimation (MLE) is a function that measures the probability of observing the given data under different parameter values of a statistical model. For a set of independent and identically distributed (i.i.d) data points \( X = \{x_1, x_2, \ldots, x_N\} \), the likelihood of a parameter \( w \) is defined as:

\[
L(w) = \prod_{i=1}^{N} P_{\text{model}}(x_i; w)
\]

where \( P_{\text{model}}(x_i; w) \) is the probability of observing the specific data point \( x_i \) under the model parameterized by \( w \).

In MLE, we seek the parameter \( w \) that maximizes this likelihood function, which is equivalent to maximizing the probability of observing the given data. While the likelihood itself is not a probability distribution, it serves as a scoring function that indicates how well the model with a particular set of parameters explains the observed data. Maximizing the likelihood function leads to finding the parameter values that make the observed data most probable under the assumed model.


\section{Q3.4 Describe maximum likelihood estimation, as minimizing NLL, cross-entropy, and KL divergence}

Let \( X = \{x_1, x_2, \ldots, x_N\} \) be training data drawn independently from the data-generating distribution \( p_{\text{data}} \). We denote the empirical data distribution as \( \hat{p}_{\text{data}} \), where \( \hat{p}_{\text{data}}(x) = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}[x_i = x] \). Let \( p_{\text{model}}(x; w) \) be a family of distributions.

The maximum likelihood estimation of \( w \) is:

\[
w_{\text{MLE}} = \arg \max_w p_{\text{model}}(X; w) = \arg \max_w \prod_{i=1}^{N} p_{\text{model}}(x_i; w)
\]

\[
= \arg \min_w -\sum_{i=1}^{N} \log p_{\text{model}}(x_i; w)
\]

\[
= \arg \min_w \mathbb{E}_{x \sim \hat{p}_{\text{data}}}[- \log p_{\text{model}}(x; w)]
\]

\[
= \arg \min_w H(\hat{p}_{\text{data}}(x), p_{\text{model}}(x; w))
\]

\[
= \arg \min_w D_{KL}(\hat{p}_{\text{data}}(x) || p_{\text{model}}(x; w)) + H(\hat{p}_{\text{data}}(x))
\]

For MLE generalized to the conditional case, where the goal is to predict \( t \) given \( x \):

\[
w_{\text{MLE}} = \arg \max_w p_{\text{model}}(t | x; w) = \arg \max_w \prod_{i=1}^{N} p_{\text{model}}(t_i | x_i; w)
\]

\[
= \arg \min_w -\sum_{i=1}^{N} \log p_{\text{model}}(t_i | x_i; w)
\]

\[
= \arg \min_w \mathbb{E}_{(x,t) \sim \hat{p}_{\text{data}}}[- \log p_{\text{model}}(t | x; w)]
\]

\[
= \arg \min_w H(\hat{p}_{\text{data}}(t | x), p_{\text{model}}(t | x; w))
\]

\[
= \arg \min_w D_{KL}(\hat{p}_{\text{data}}(t | x) || p_{\text{model}}(t | x; w)) + H(\hat{p}_{\text{data}}(t | x))
\]

Where \( H(\hat{p}_{\text{data}}) \) is the entropy of the empirical data distribution and \( D_{KL} \) is the Kullback-Leibler divergence. The terms are defined such that the conditional entropy is \( H(\hat{p}_{\text{data}}) = \mathbb{E}_{(x,t) \sim \hat{p}_{\text{data}}}[-\log(\hat{p}_{\text{data}}(t | x))] \) and the conditional cross-entropy is \( H(\hat{p}_{\text{data}}, p_{\text{model}}) = \mathbb{E}_{(x,t) \sim \hat{p}_{\text{data}}}[-\log(p_{\text{model}}(t | x; w))] \). The negative log-likelihood (NLL) is equivalent to cross-entropy or Kullback-Leibler divergence in the context of MLE.


\section{Q3.5 Considering binary logistic regression model, write down its parameters (including their size) and explain how prediction is performed (including the formula for the sigmoid function). Describe how we can interpret the outputs of the linear part of the model as logits}

In a binary logistic regression model, the prediction \( \hat{y} \) is based on the probability that a given input \( x \) belongs to a particular class \( C_1 \), which is modeled using the logistic function \( \sigma \). The parameters of the model include:

\begin{itemize}
    \item Weight vector \( w \in \mathbb{R}^D \), where \( D \) is the number of features.
    \item Bias \( b \in \mathbb{R} \).
\end{itemize}

The logistic regression model makes predictions using the sigmoid function \( \sigma \) applied to the linear combination of the input features and the model weights:

\[
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{e^z + 1}
\]

Given an input \( x \), the linear part of the logistic regression model computes \( z \) as:

\[
z = x^T w + b
\]

The final prediction \( \hat{y} \) is given by:

\[
\hat{y}(x; w) = \sigma(z) = \sigma(x^T w + b)
\]

The output of the linear part \( x^T w + b \) can be interpreted as logits, which are the log odds of the probability that \( x \) belongs to class \( C_1 \) before the sigmoid transformation. Logits can take any real value, and transforming them through the sigmoid function maps them to the \((0, 1)\) interval, representing probabilities.

\section{Q3.6 Write down an L2-regularized minibatch SGD algorithm for training a binary logistic regression model, including the explicit formulas of the loss function and its gradient}

To train the logistic regression, we use MLE (maximum likelihood estimation). The loss for a minibatch $\mathcal{X} = \{(\vec{x}_1, t_1), (\vec{x}_2, t_2), \ldots, (\vec{x}_N, t_N)\}$ is given by
$$
E({w}) = \frac{1}{N} \sum_{i} -\log(p(C_{t_i} | {x}_i; {w})) + \frac{\lambda}{2} \| {w} \|^2
$$
The logistic regression model uses the sigmoid function as the activation function, which is defined as
$$
\sigma(x) = \frac{1}{1 + e^{-x}}.
$$
The parameters are updated using the SGD algorithm as follows:
\begin{enumerate}
  \item Initialize the weight vector $\vec{w} \leftarrow \vec{0}$ or randomly.
  \item Repeat until convergence:
  \begin{itemize}
    \item Compute the gradient for a minibatch $\mathcal{B}$:
    $$
    \vec{g} \leftarrow \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\vec{w}} \left(-\log(p(C_{t_i} | \vec{x}_i; \vec{w}))\right) + \lambda \vec{w}
    $$
    \item Update the weights:
    $$
    \vec{w} \leftarrow \vec{w} - \alpha \vec{g}
    $$
  \end{itemize}
\end{enumerate}

\subsection*{Logistic Regression Gradient}
Consider the log-likelihood of logistic regression $\log p(t|\vec{x}; \vec{w})$. For brevity, we denote $\bar{y}(\vec{x}; \vec{w}) = \vec{x}^T \vec{w}$ simply as $\bar{y}$ in the following computation.

Given that for $t \sim \text{Ber}(\phi)$ we have $p(t) = \phi^t (1-\phi)^{1-t}$, we can rewrite the log-likelihood as:

\[
\log p(t|\vec{x}; \vec{w}) = \log \sigma(\bar{y})^t (1 - \sigma(\bar{y}))^{1-t}
\]

This simplifies to:

\[
t \cdot \log (\sigma(\bar{y})) + (1 - t) \cdot \log (1 - \sigma(\bar{y}))
\]

The gradient of the logistic regression's likelihood with respect to the weights $\vec{w}$ is derived as follows:
\begin{align*}
\nabla_{\vec{w}} -\log p(t|\vec{x}; \vec{w}) &= \nabla_{\vec{w}} \left( -t \log (\sigma(\hat{y})) - (1-t) \log (1 - \sigma(\hat{y})) \right) \\
&= \nabla_{\vec{w}} \left( -t \log (\sigma(\vec{x}^T \vec{w})) - (1-t) \log (1 - \sigma(\vec{x}^T \vec{w})) \right) \\
&= -t \cdot \frac{1}{\sigma(\hat{y})} \cdot \nabla_{\vec{w}} \sigma(\hat{y}) + (1-t) \cdot \frac{1}{1 - \sigma(\hat{y})} \cdot \nabla_{\vec{w}} (-\sigma(\hat{y})) \\
&= \left( -t + t \sigma(\hat{y}) + \sigma(\hat{y}) - t \sigma(\hat{y}) \right) \vec{x} \\
&= \left( \sigma(\vec{x}^T \vec{w}) - t \right) \vec{x}
\end{align*}
where $\hat{y} = \vec{x}^T \vec{w}$ and the gradient of the sigmoid function $\sigma(x)$ is $\sigma(x)(1 - \sigma(x))$. The resulting gradient is used to update the weights in the SGD algorithm.

Therefor the gradient of the loss function is:
\[
\nabla_{\vec{w}} E(\vec{w}) = \frac{1}{N} \sum_{i} \left( \sigma(\vec{x}_i^T \vec{w}) - t_i \right) \vec{x}_i + \lambda \vec{w}
\]


\part{Lecture 4}
\section{Q4.1 Define mean squared error and show how it can be derived using MLE}

Mean Squared Error (MSE) is commonly used as a loss function for regression problems and can be derived from Maximum Likelihood Estimation (MLE) when we assume that the target variables, \( t \), conditioned on the inputs, \( \vec{x} \), are normally distributed with a mean equal to the output of the model, \( y(\vec{x}; \vec{w}) \), and variance \( \sigma^2 \). Under this assumption, the probability distribution for \( t \) is given by \( p(t|\vec{x}; \vec{w}) = \mathcal{N}(t; y(\vec{x}; \vec{w}), \sigma^2) \).

Applying MLE, we look for the parameters \( \vec{w} \) that maximize the likelihood of the observed data, which is equivalent to minimizing the negative log-likelihood. This leads to the MSE as follows:

\[
\begin{aligned}
\vec{w}_{\text{MLE}} &= \arg \max_{\vec{w}} p(\vec{t}|\vec{X}; \vec{w}) = \arg \min_{\vec{w}} \sum_{i=1}^{N} -\log p(t_i|\vec{x}_i; \vec{w}) \\
&= \arg \min_{\vec{w}} -\sum_{i=1}^{N} \log \left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(t_i - y(\vec{x}_i; \vec{w}))^2}{2\sigma^2}\right)\right) \\
&= \arg \min_{\vec{w}} -N \log \left((2\pi\sigma^2)^{-\frac{1}{2}}\right) - \sum_{i=1}^{N} \frac{(t_i - y(\vec{x}_i; \vec{w}))^2}{2\sigma^2} \\
&= \arg \min_{\vec{w}} \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y(\vec{x}_i; \vec{w}) - t_i)^2.
\end{aligned}
\]

Ignoring the constant \( \frac{1}{2\sigma^2} \), we obtain the familiar form of the MSE:

\[
E(\vec{w}) = \frac{1}{N} \sum_{i=1}^{N} (y(\vec{x}_i; \vec{w}) - t_i)^2.
\]

This derivation shows that when we assume a normal distribution for the model errors, the MLE approach naturally leads to the MSE as the loss function to be minimized.


\section{Q4.2 Considering KK-class logistic regression model, write down its parameters (including their size) and explain how prediction is performed (including the formula for the softmax function). Describe how we can interpret the outputs of the linear part of the model as logits}

To extend the binary logistic regression to a \( K \)-class case, we define the model parameters as a weight matrix \( W \in \mathbb{R}^{D \times K} \), where \( D \) is the number of features and \( K \) is the number of classes. Each column \( W_{*,i} \) corresponds to the weights associated with class \( i \).

Predictions are made using the softmax function applied to the linear outputs, known as logits. For an input vector \( \vec{x} \), the logits are given by \( \vec{y}(\vec{x}; W) = W^T\vec{x} \), and the softmax function is defined as:
\[
\text{softmax}(\vec{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\]
for each class \( i \), where \( \vec{z} \) is the vector of logits. 

Therefore, the probability that \( \vec{x} \) belongs to class \( i \) is:
\[
p(C_i | \vec{x}; W) = \text{softmax}(\vec{y}(\vec{x}; W))_i = \frac{e^{\vec{x}^TW_{*,i}}}{\sum_{j=1}^{K} e^{\vec{x}^TW_{*,j}}}
\]

The linear part of the model \( \vec{x}^TW \) can be interpreted as logits because they represent the log odds before passing through the softmax function. The softmax function normalizes these log odds to probabilities that sum to one across all classes.

Training a \( K \)-class logistic regression model typically involves using the cross-entropy loss function, which for a given data point \( (x_i, t_i) \) is:
\[
E(W) = -\sum_{i=1}^{N} \log p(C_{t_i} | \vec{x}_i; W)
\]

This loss function is minimized using optimization algorithms such as minibatch stochastic gradient descent (SGD).


\section{
Q4.3 Explain the relationship between the sigmoid function and softmax
}

The softmax function is a generalization of the sigmoid function to the case where there are multiple classes. For binary classification (\(K=2\)), the softmax function simplifies to the sigmoid function. Specifically, the sigmoid function is defined as:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]
which is the probability of a single class (e.g., class 1 in binary classification).

The softmax function, which is used for multinomial logistic regression with \(K\) classes, is defined as:
\[
\text{softmax}(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\]
for each class \(i\). When \(K=2\), this reduces to:
\[
\text{softmax}([x, 0]) = \frac{e^x}{e^x + e^0} = \frac{1}{1 + e^{-x}}
\]
which is identical to the sigmoid function. Therefore, the softmax function can be seen as an extension of the sigmoid function from binary to multiclass classification, where the output for each class is the normalized exponential function of the logits, ensuring that the class probabilities sum to one.

The sigmoid function thus can be represented as a softmax function applied to a vector with two elements, where one element is the logit \(x\) and the other is zero. This connection shows the versatility of softmax as a multi-class sigmoid function.

\section{Q4.4 L2-Regularized Minibatch SGD for K-Class Logistic Regression}

To train a \( K \)-class logistic regression model, we use the minibatch stochastic gradient descent (SGD) with L2 regularization. The loss function is the regularized negative log-likelihood, and the gradient takes into account the regularization term.

\subsection*{Algorithm}
\begin{enumerate}
    \item \textbf{Input:} Input dataset \( \mathbf{X} \in \mathbb{R}^{N \times D} \), target labels \( \mathbf{t} \in \{0, 1, \ldots, K-1\}^N \), learning rate \( \alpha \in \mathbb{R}^+ \), and regularization parameter \( \lambda \in \mathbb{R}^+ \).
    
    \item \textbf{Model Parameters:} Initialize weight matrix \( \mathbf{W} \in \mathbb{R}^{D \times K} \) and bias vector \( \mathbf{b} \in \mathbb{R}^K \) either to zero or with random values.
    
    \item \textbf{Optimization:} Repeat until convergence:
    \begin{itemize}
        \item Compute the gradient of the loss with respect to \( \mathbf{W} \) for a minibatch \( \mathcal{B} \):
        \[
        \mathbf{g} \leftarrow \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\mathbf{W}} \left( -\log(p(C_{t_i} | \mathbf{x}_i; \mathbf{W})) \right) + \lambda \mathbf{W}
        \]
        
        \item Update the weights:
        \[
        \mathbf{W} \leftarrow \mathbf{W} - \alpha \mathbf{g}
        \]
    \end{itemize}
\end{enumerate}

\subsection*{Loss Function}
The regularized loss function for a minibatch \( \mathcal{B} \) is:
\[
E(\mathbf{W}) = -\frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \log(p(C_{t_i} | \mathbf{x}_i; \mathbf{W})) + \frac{\lambda}{2} \|\mathbf{W}\|^2_F
\]
where \( \|\mathbf{W}\|^2_F \) is the Frobenius norm of \( \mathbf{W} \), representing the L2 regularization term.

\subsection*{Gradient}
The gradient of the loss function with L2 regularization is:
\[
\nabla_{\mathbf{W}} E(\mathbf{W}) = -\frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} (p(C_{t_i} | \mathbf{x}_i; \mathbf{W}) - 1_t) \mathbf{x}_i^T + \lambda \mathbf{W}
\]
Note that $1_t$ is a one-hot vector with a 1 in the position corresponding to the target class $t$.


\section{Q4.5 Convexity of Decision Regions in Multiclass Logistic Regression}

To prove the convexity of decision regions in multiclass logistic regression, consider two points \( x_A \) and \( x_B \) in the same decision region \( R_k \). The decision criterion for logistic regression is based on the linear functions \( x^T W \), where \( W \) is the weight matrix. A point \( x \) is in region \( R_k \) if and only if

\[ \hat{y}(x)_k = x^T W_k \]

is the largest among all class scores. For two points \( x_A, x_B \in R_k \), and any \( \lambda \in [0, 1] \), their convex combination \( x = \lambda x_A + (1 - \lambda)x_B \) also satisfies

\[ \hat{y}(x)_k = \lambda \hat{y}(x_A)_k + (1 - \lambda)\hat{y}(x_B)_k \]

Given that both \( \hat{y}(x_A)_k \) and \( \hat{y}(x_B)_k \) are the largest scores for their respective points, \( \hat{y}(x)_k \) will also be the largest score for the convex combination, placing \( x \) in \( R_k \). This holds for any convex combination of points in \( R_k \), thus \( R_k \) is convex.


\section{Q4.6 Considering a single-layer MLP with D input neurons, H hidden neurons, K output neurons, hidden activation f, and output activation a, list its parameters (including their size) and write down how the output is computed.}

A single-layer Multilayer Perceptron (MLP) with \( D \) input neurons, \( H \) hidden neurons, and \( K \) output neurons, uses the following parameters:

\begin{itemize}
    \item Hidden layer weights \( W^{(h)} \in \mathbb{R}^{D \times H} \)
    \item Hidden layer biases \( b^{(h)} \in \mathbb{R}^{H} \)
    \item Output layer weights \( W^{(y)} \in \mathbb{R}^{H \times K} \)
    \item Output layer biases \( b^{(y)} \in \mathbb{R}^{K} \)
\end{itemize}

The activation functions for the hidden and output layers are denoted by \( f \) and \( a \) respectively. The output is computed as follows:

For a single input \( x \in \mathbb{R}^{D} \):
\begin{enumerate}
    \item Hidden layer activations: \( h = f(x^T W^{(h)} + b^{(h)}) \)
    \item Output predictions: \( y = a(h^T W^{(y)} + b^{(y)}) \)
\end{enumerate}

For a batch of inputs \( X \in \mathbb{R}^{N \times D} \):
\begin{enumerate}
    \item Batch hidden layer activations: \( H = f(X W^{(h)} + 1b^{(h)}) \)
    \item Batch output predictions: \( Y = a(H W^{(y)} + 1b^{(y)}) \)
\end{enumerate}
where \( 1 \) is a column vector of ones for bias broadcasting over the batch.

\section{Q4.7 List the definitions of frequently used MLP output layer activations (the ones producing parameters of a Bernoulli distribution and a categorical distribution). Then write down three commonly used hidden layer activations (sigmoid, tanh, ReLU)}

\subsection*{Output Layer Activations}
\begin{itemize}
    \item \textbf{Identity (Regression):} \( \text{identity}(x) = x \)
    \item \textbf{Sigmoid (Binary Classification):} \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
    \item \textbf{Softmax (K-class Classification):} \( \text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)
\end{itemize}

\subsection*{Hidden Layer Activations}
\begin{itemize}
    \item \textbf{Sigmoid:} \( \sigma(x) = \frac{1}{1 + e^{-x}} \)
    \item \textbf{Tanh:} \( \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = 2\sigma(2x)-1\)
    \item \textbf{ReLU:} \( \text{ReLU}(x) = \max(0, x) \)
\end{itemize}

\part{Lecture 5}

\section{Q5.1 Considering a single-layer MLP with D input neurons, a ReLU hidden layer with H units and a softmax output layer with K units, write down the explicit formulas of the gradient of all the MLP parameters (two weight matrices and two bias vectors), assuming input x, target t, and negative log likelihood loss}

Assuming an MLP with \( D \) input neurons, a ReLU hidden layer with \( H \) units, and a softmax output layer with \( K \) units, we compute the gradients of the loss \( L \) with respect to the weight matrices \( W^{(h)}, W^{(y)} \) and bias vectors \( b^{(h)}, b^{(y)} \) given an input \( x \), a target \( t \), and using the negative log likelihood loss.

Let \( x \in \mathbb{R}^D \) be the input vector, \( h \in \mathbb{R}^H \) be the output of the hidden layer, and \( y \in \mathbb{R}^K \) be the output of the network. The negative log likelihood loss for a correct class \( c \) is given by \( L = -\log(y_c) = -\log(p(C | x))\).

\subsection*{Forward Pass:}
\begin{align*}
h^{(in)} &= x^T W^{(h)} + b^{(h)} \\
h &= \text{ReLU}(h^{(in)}) \\
y^{(in)} &= h^T W^{(y)} + b^{(y)} \\
y &= \text{softmax}(y^{(in)})
\end{align*}

\subsection*{Backward Pass (Gradients):}
\begin{align*}
\frac{\partial L}{\partial y_k} &= -\frac{t_k}{y_k} \\
\frac{\partial L}{\partial \mathbf{y}^{(in)}} &= \mathbf{y} - \mathbf{t} \quad \text{(since } \sum_k t_k = 1 \text{)} \\
\frac{\partial L}{\partial \mathbf{W}^{(y)}} &= \mathbf{h} \left(\frac{\partial L}{\partial \mathbf{y}^{(in)}}\right)^\top \\
\frac{\partial L}{\partial \mathbf{b}^{(y)}} &= \frac{\partial L}{\partial \mathbf{y}^{(in)}} \\
\frac{\partial L}{\partial \mathbf{h}} &= \mathbf{W}^{(y)} \frac{\partial L}{\partial \mathbf{y}^{(in)}}^\top \\
\frac{\partial L}{\partial \mathbf{h}^{(in)}} &= \begin{cases}
\frac{\partial L}{\partial \mathbf{h}} & \text{if } \mathbf{h}^{(in)} > 0 \\
0 & \text{otherwise}
\end{cases} \quad \text{(ReLU gradient)} \\
\frac{\partial L}{\partial \mathbf{W}^{(h)}} &= \mathbf{x} \left(\frac{\partial L}{\partial \mathbf{h}^{(in)}}\right)^\top \\
\frac{\partial L}{\partial \mathbf{b}^{(h)}} &= \frac{\partial L}{\partial \mathbf{h}^{(in)}}
\end{align*}

\subsection*{Gradient of Loss with respect to Output Probabilities \( \mathbf{y} \)}
\begin{equation*}
\frac{\partial L}{\partial \mathbf{y}} = - \frac{\mathbf{t}}{\mathbf{y}}
\end{equation*}

\subsection*{Gradient of Output Probabilities with respect to Logits \( \mathbf{y}^{(\text{in})} \)}
The softmax function for a class \( k \) is given by \( y_k = \frac{e^{y^{(\text{in})}_k}}{\sum_j e^{y^{(\text{in})}_j}} \). Its derivative with respect to the logits \( y^{(\text{in})}_i \) is:
\begin{align*}
\frac{\partial y_k}{\partial y^{(\text{in})}_i} &= \begin{cases}
y_k (1 - y_i) & \text{if } i = k, \\
-y_k y_i & \text{if } i \neq k.
\end{cases}
\end{align*}

\subsection*{Chain Rule Application for Loss Gradient with respect to Logits}
\begin{align*}
\frac{\partial L}{\partial y^{(\text{in})}_i} &= \sum_k \frac{\partial L}{\partial y_k} \frac{\partial y_k}{\partial y^{(\text{in})}_i} \\
&= \frac{\partial L}{\partial y_i} \frac{\partial y_i}{\partial y^{(\text{in})}_i} + \sum_{k \neq i} \frac{\partial L}{\partial y_k} \frac{\partial y_k}{\partial y^{(\text{in})}_i} \\
&= -\frac{t_i}{y_i} y_i (1 - y_i) - \sum_{k \neq i} \frac{t_k}{y_k} (-y_k y_i) \\
&= -t_i + t_i y_i + \sum_{k \neq i} t_k y_i \\
&= -t_i + y_i \left(t_i + \sum_{k \neq i} t_k\right) \\
&= -t_i + y_i \sum_k t_k \\
&= y_i - t_i \quad \text{(since \( \sum_k t_k = 1 \) for one-hot encoded targets)}
\end{align*}

\subsection*{Gradient of Loss with respect to Output Layer Weights \( \mathbf{W}^{(y)} \)}
\begin{align*}
\frac{\partial L}{\partial \mathbf{W}^{(y)}} &= \frac{\partial L}{\partial \mathbf{y}^{(\text{in})}} \frac{\partial \mathbf{y}^{(\text{in})}}{\partial \mathbf{W}^{(y)}} \\
&= \left( \mathbf{y} - \mathbf{t} \right)^\top \mathbf{h}
\end{align*}

\subsection*{Gradient of Loss with respect to Output Layer Biases \( \mathbf{b}^{(y)} \)}
\begin{equation*}
\frac{\partial L}{\partial \mathbf{b}^{(y)}} = \mathbf{y} - \mathbf{t}
\end{equation*}

\subsection*{Gradient of Loss with respect to Hidden Layer Outputs \( \mathbf{h} \)}
\begin{equation*}
\frac{\partial L}{\partial \mathbf{h}} = \mathbf{W}^{(y)} \left( \mathbf{y} - \mathbf{t} \right)
\end{equation*}

\subsection*{Gradient of Loss with respect to Hidden Layer Pre-Activation \( \mathbf{h}^{(\text{in})} \)}
\begin{equation*}
\frac{\partial L}{\partial \mathbf{h}^{(\text{in})}} = \frac{\partial L}{\partial \mathbf{h}} \cdot \mathbb{I}(\mathbf{h}^{(\text{in})} > 0)
\end{equation*}
where \( \mathbb{I} \) is the indicator function, yielding 1 for elements where the condition is true and 0 otherwise, which corresponds to the derivative of the ReLU activation function.

\subsection*{Gradient of Loss with respect to Hidden Layer Weights \( \mathbf{W}^{(h)} \)}
\begin{align*}
\frac{\partial L}{\partial \mathbf{W}^{(h)}} &= \mathbf{x}^\top \frac{\partial L}{\partial \mathbf{h}^{(\text{in})}}
\end{align*}

\subsection*{Gradient of Loss with respect to Hidden Layer Biases \( \mathbf{b}^{(h)} \)}
\begin{equation*}
\frac{\partial L}{\partial \mathbf{b}^{(h)}} = \frac{\partial L}{\partial \mathbf{h}^{(\text{in})}}
\end{equation*}

\subsection*{Update Rules:}
\begin{align*}
W^{(h)} &= W^{(h)} - \alpha \frac{\partial L}{\partial W^{(h)}} \\
b^{(h)} &= b^{(h)} - \alpha \frac{\partial L}{\partial b^{(h)}} \\
W^{(y)} &= W^{(y)} - \alpha \frac{\partial L}{\partial W^{(y)}} \\
b^{(y)} &= b^{(y)} - \alpha \frac{\partial L}{\partial b^{(y)}}
\end{align*}

In these equations, \( \alpha \) represents the learning rate, and the derivatives with respect to \( h^{(in)} \) take into account the ReLU activation, which is zero for negative inputs and equal to the derivative of the loss with respect to \( h \) otherwise. The derivative with respect to \( y^{(in)} \) is computed as the difference between the output probabilities \( y \) and the one-hot encoded target vector \( t \).

\section*{Q5.2 Formulate Universal Approximation Theorem ('89)}

Let \(\phi(x) : \mathbb{R} \rightarrow \mathbb{R}\) be a nonconstant, bounded and nondecreasing continuous function. (Later a proof was given also for \(\phi = \text{ReLU}\) and even for any nonpolynomial function.)

For any \(\epsilon > 0\) and any continuous function \(f : [0, 1]^D \rightarrow \mathbb{R}\), there exists \(H \in \mathbb{N}\), \(\mathbf{v} \in \mathbb{R}^H\), \(\mathbf{b} \in \mathbb{R}^H\) and \(\mathbf{W} \in \mathbb{R}^{D \times H}\), such that if we denote

\[
F(\mathbf{x}) = \mathbf{v}^T \phi(\mathbf{W}^T \mathbf{x} + \mathbf{b}) = \sum_{i=1}^{H} v_i \phi(\mathbf{x}^T \mathbf{W}_{*,i} + b_i),
\]

where \(\phi\) is applied elementwise, then for all \(\mathbf{x} \in [0, 1]^D\):

\[
\left| F(\mathbf{x}) - f(\mathbf{x}) \right| < \epsilon.
\]

\section{Q5.2 How do we search for a minimum of a function $f(\mathbf{x}) : \mathbb{R}^D \rightarrow \mathbb{R}$ subject to equality constraints $g_1(\mathbf{x}) = 0, \ldots, g_m(\mathbf{x}) = 0$?}

We search for a minimum of a function $f(\mathbf{x}) : \mathbb{R}^D \rightarrow \mathbb{R}$ subject to equality constraints $g_1(\mathbf{x}) = 0, \ldots, g_m(\mathbf{x}) = 0$ using the method of Lagrange multipliers. This involves finding a point $\mathbf{x} \in \mathbb{R}^D$ and a set of multipliers $\lambda_1, \ldots, \lambda_m \in \mathbb{R}$ such that the gradient of the Lagrangian function $\mathcal{L}(\mathbf{x}, \mathbf{\lambda})$ with respect to both $\mathbf{x}$ and $\mathbf{\lambda}$ is zero.

The Lagrangian is defined as:
\[
\mathcal{L}(\mathbf{x}, \mathbf{\lambda}) = f(\mathbf{x}) - \sum_{i=1}^{m} \lambda_i g_i(\mathbf{x}),
\]
where $\nabla_{\mathbf{x}}\mathcal{L}(\mathbf{x}, \mathbf{\lambda}) = 0$ and $\nabla_{\mathbf{\lambda}}\mathcal{L}(\mathbf{x}, \mathbf{\lambda}) = 0$. This gives us a system of equations which, when solved, gives the points $\mathbf{x}$ that minimize $f(\mathbf{x})$ subject to the constraints.


\section{Q5.3 Prove which categorical distribution with $N$ classes has maximum entropy}

We want to find a categorical distribution \( \mathbf{p} = (p_1, \ldots, p_N) \) that maximizes entropy, subject to the constraints:

\begin{itemize}
    \item \( p_i \geq 0 \) for all \( i \),
    \item \( \sum_{i=1}^{N} p_i = 1 \).
\end{itemize}

The entropy \( H(\mathbf{p}) \) for a categorical distribution is given by:

\[ H(\mathbf{p}) = -\sum_{i=1}^{N} p_i \log(p_i) \]

We form the Lagrangian \( \mathcal{L} \) to include the equality constraint:

\[ \mathcal{L}(\mathbf{p}, \lambda) = -\sum_{i=1}^{N} p_i \log(p_i) + \lambda \left( \sum_{i=1}^{N} p_i - 1 \right) \]

Taking the derivative of \( \mathcal{L} \) with respect to \( p_i \) and setting it to zero:

\[ 0 = \frac{\partial \mathcal{L}}{\partial p_i} = -\log(p_i) - 1 + \lambda \]

Solving for \( p_i \), we get:

\[ p_i = e^{\lambda - 1} \]

Since all \( p_i \) must satisfy the equality constraint \( \sum_{i=1}^{N} p_i = 1 \), substituting \( p_i \) gives:

\[ \sum_{i=1}^{N} e^{\lambda - 1} = 1 \]

\[ N e^{\lambda - 1} = 1 \]

\[ e^{\lambda - 1} = \frac{1}{N} \]

\[ p_i = \frac{1}{N} \]

Therefore, each \( p_i \) is \( \frac{1}{N} \), indicating that the distribution with maximum entropy is the uniform distribution.

\section{Q5.4 Consider derivation of softmax using maximum entropy principle, assuming we have a dataset of $N$ examples $(x_i,t_i)$, $x_i \in \mathbb{R}^D$, $t_i \in \{1,2,\ldots,K\}$. Formulate the three conditions we impose on the searched $\pi:\mathbb{R}^D \rightarrow \mathbb{R}^K$, and write down the Lagrangian to be minimized.}

Given a dataset of $N$ examples $(x_i, t_i)$ where $x_i \in \mathbb{R}^D$ and $t_i \in \{1, 2, \ldots, K\}$, we want to derive a softmax function using the maximum entropy principle. The softmax function $\pi(x)$ must satisfy the following conditions:

\begin{enumerate}
    \item For all $x \in \mathbb{R}^D$ and each class $k$, the predicted probability $\pi(x)_k \geq 0$.
    \item For each input $x$, the probabilities must sum up to 1: $\sum_{k=1}^K \pi(x)_k = 1$.
    \item The expected value of the predicted distribution should match the empirical distribution: $\frac{1}{N} \sum_{i=1}^N \pi(x_i)_k = \frac{1}{N} \sum_{i=1}^N [t_i = k]$ for each class $k$.
\end{enumerate}

The Lagrangian $\mathcal{L}$, incorporating these constraints with Lagrange multipliers $\lambda$ and $\mu_k$.

We want to minimize $-\sum_{i=1}^{N} H(\pi(x_i))$ given
\begin{itemize}
    \item for $1 \leq i \leq N$, $1 \leq k \leq K$: $\pi(x_i)_k \geq 0$,
    \item for $1 \leq i \leq N$: $\sum_{k=1}^{K} \pi(x_i)_k = 1$,
    \item for $1 \leq j \leq D$, $1 \leq k \leq K$: $\sum_{i=1}^{N} \pi(x_i)_k x_{i,j} = \sum_{i=1}^{N} [t_i = k] x_{i,j}$.
\end{itemize}

We therefore form a Lagrangian (ignoring the first inequality constraint):
\[
\mathcal{L} = \sum_{i=1}^{N} \sum_{k=1}^{K} \pi(x_i)_k \log(\pi(x_i)_k) 
- \sum_{j=1}^{D} \sum_{k=1}^{K} \lambda_{j,k} \left( \sum_{i=1}^{N} \pi(x_i)_k x_{i,j} - [t_i = k] x_{i,j} \right) 
- \sum_{i=1}^{N} \beta_i \left( \sum_{k=1}^{K} \pi(x_i)_k - 1 \right).
\]


\section{Q5.5 Define precision (including true positives and others), recall, $F1$ score, and $F\beta$ score (we stated several formulations for $F1$ and $F\beta$ scores; any one of them will do)}

The confusion matrix is a table used to describe the performance of a classification model:

\begin{table}[h]
\centering
\begin{tabular}{l|l|l}
& \textbf{Predicted Positive} & \textbf{Predicted Negative} \\ \hline
\textbf{Actual Positive} & True Positives (TP) & False Negatives (FN) \\ \hline
\textbf{Actual Negative} & False Positives (FP) & True Negatives (TN) \\
\end{tabular}
\caption{Confusion Matrix}
\label{table:confusion_matrix}
\end{table}

Precision quantifies the number of correct positive predictions made. It is defined as:
\[ \text{Precision} = \frac{\text{True Positives (TP)}}{\text{TP} + \text{False Positives (FP)}} \]

Recall measures the proportion of actual positives correctly identified. It is defined as:
\[ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{False Negatives (FN)}} \]

The \( F_1 \) score is the harmonic mean of precision and recall. It is defined as:
\[ F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]

The \( F_{\beta} \) score generalizes the \( F_1 \) score by weighing recall more heavily than precision. It is defined as:
\[ F_{\beta} = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{(\beta^2 \times \text{Precision}) + \text{Recall}} \]

\section{Q5.6 Explain the difference between micro-averaged and macro-averaged $F1$ scores. }

The micro-averaged \( F_1 \) score (\( F_1^{\mu} \)) aggregates the contributions of all classes to compute the average score. It is given by:
\[ F_1^{\mu} =2\times \frac{Precision_{micro}\times Recall_{micro}}{Precision_{micro}+Recall_{micro}} = \frac{2 \sum TP}{2 \sum TP + \sum FP + \sum FN} \]

The macro-averaged \( F_1 \) score (\( F_1^{M} \)) is the arithmetic mean of the \( F_1 \) scores per class, treating all classes equally:
\[ F_1^{M} = \frac{1}{K} \sum_{k=1}^{K} F_{1k} \]
where \( F_{1k} \) is the \( F_1 \) score of the \( k \)-th class.

\section{Q5.7 Explain (using examples) why accuracy is not a suitable metric for unbalanced target classes, e.g., for a diagnostic test for a contagious disease}

Accuracy is defined as the ratio of correctly predicted observations to the total observations. In the context of unbalanced datasets, particularly in disease diagnosis where the disease prevalence is low, a model might predict "no disease" for all patients and still achieve high accuracy. For example:

\[
\text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Observations}}
\]

Consider a dataset with 1000 individuals where only 10 have the disease. A model that predicts "no disease" for everyone would have an accuracy of:

\[
\frac{0 + 990}{10 + 990} = \frac{990}{1000} = 99\%
\]

Despite the high accuracy, the model fails to detect any true disease cases, demonstrating the inadequacy of accuracy as a performance metric in such scenarios. It is more informative to look at metrics such as precision, recall, and the \( F_1 \) score in cases of class imbalance.

\part{Lecture 6}

\section{
Q6.1 Explain how is the TF-IDF weight of a given document-term pair computed
}

The TF-IDF weight of a document-term pair is given by:

\[
\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t,D)
\]

where:
\begin{itemize}
  \item \( \text{TF}(t,d) \) is the term frequency, defined as the number of times term \( t \) appears in document \( d \), normalized or not.
  \item \( \text{IDF}(t,D) \) is the inverse document frequency, calculated as:

  \[
  \text{IDF}(t,D) = \log \frac{N}{|\{d \in D : t \in d\}| (optionaly + 1))}
  \]

  In this formula, \( N \) is the total number of documents in the corpus \( D \), and \( |\{d \in D : t \in d\}| \) is the number of documents where the term \( t \) appears (i.e., \( \text{df}_t \), the document frequency of \( t \)).
\end{itemize}

The TF-IDF score increases with the number of times a word appears in the document but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general.

\section{Q6.2 Define conditional entropy, mutual information, write down the relation between them, and finally prove that mutual information is zero if and only if the two random variables are independent (you do not need to prove statements about $D_{KL}$)}

Conditional entropy $H(Y|X)$ quantifies the expected value of the entropy of $Y$ given that the value of $X$ is known. It is defined as:

\[
H(Y|X) = -\sum_{x \in X, y \in Y} P(x, y) \log P(y|x).
\]

Mutual information $I(X; Y)$ measures the amount of information that one random variable contains about another random variable. It is defined as:

\[
I(X; Y) = \sum_{x \in X, y \in Y} P(x, y) \log \frac{P(x, y)}{P(x)P(y)}.
\]

The relationship between conditional entropy and mutual information can be expressed as:

\[
I(X; Y) = H(Y) - H(Y|X).
\]

This equation implies that mutual information is the reduction in uncertainty about $Y$ due to the knowledge of $X$.

The mutual information is symmetrical, so
\[
I(X; Y) = I(Y; X) = H(X) - H(X|Y) = H(Y) - H(Y|X).
\]

Therefore
\[
    I(X;Y) = D_{KL}(P(X,Y)||P(X)P(Y))
\]

\section{Q6.3 Show that TF-IDF terms can be considered portions of suitable mutual information
}

Let $\mathcal{D}$ be a collection of documents and $\mathcal{T}$ a collection of terms. We can express the mutual information between a document $d$ and a term $t$ using their probabilities and the TF-IDF measure.

\begin{itemize}
  \item The probability of selecting a document $d$ uniformly at random from $\mathcal{D}$ is $P(d) = \frac{1}{|\mathcal{D}|}$.
  \item The information content of a document $I(d) = H(\mathcal{D}) = \log |\mathcal{D}|$.
  \item The probability of a term $t$ occurring in a document $d$ is $P(t|d) = \frac{|\{d \in \mathcal{D} : t \in d\}|}{|\mathcal{D}|}$.
  \item The information content of a term $t$ in a document $d$ is $I(t|d) = \log |\{d \in \mathcal{D} : t \in d\}|$.
  \item The difference in information content of a document with and without a term is the IDF: $I(d) - I(t|d) = \log |\mathcal{D}| - \log |\{d \in \mathcal{D} : t \in d\}| = \text{IDF}(t)$.
\end{itemize}

The mutual information $I(\mathcal{D}; \mathcal{T})$ is calculated as:

\[
I(\mathcal{D}; \mathcal{T}) = \sum_{d, t \in d} P(d) \cdot P(t|d) \cdot (I(d) - I(t|d)).
\]

Given the definitions of TF and IDF, we can write:

\[
I(\mathcal{D}; \mathcal{T}) = \frac{1}{|\mathcal{D}|} \sum_{d, t \in d} \text{TF}(t, d) \cdot \text{IDF}(t).
\]

Thus, we can interpret the TF-IDF weight as a portion of the mutual information between the collection of documents $\mathcal{D}$ and the collection of terms $\mathcal{T}$, where each TF-IDF value corresponds to a “bit of information” for a document-term pair.

\section{Q6.4 Explain the concept of word embedding in the context of MLP and how it relates to representation learning}

Word embedding is a technique in representation learning where words from a vocabulary are associated with vectors of real numbers, effectively capturing their semantic meanings in a continuous vector space. Semantically similar words are positioned closely in this space. 

In the realm of Multilayer Perceptrons (MLPs), these embeddings serve as the input layer. Each word is represented by a unique, learnable vector, rather than a high-dimensional, sparse one-hot vector. Throughout the training phase, the MLP fine-tunes these embeddings via backpropagation, based on the context in which words appear.

This approach is a key part of representation learning because it enables MLPs to internalize language subtleties directly from data, surpassing the need for manual feature engineering. It allows the network to capture complex syntactic and semantic word relationships, enhancing its performance on Natural Language Processing (NLP) tasks such as sentiment analysis, translation, and text categorization.

Word embeddings are the cornerstone of modern NLP models and are integrated into more advanced neural architectures like Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers, driving forward the state-of-the-art in various NLP applications.


\section{Q6.4 Describe the skip-gram model trained using negative sampling.}

The skip-gram model aims to learn word embeddings that predict the context words given a target word. For a given word in the vocabulary, the model outputs a probability distribution over all words to be the 'context' words.

The Skip-gram model with negative sampling (SGNS) enhances training efficiency by altering the objective function. Instead of predicting the presence of context words among all words in the vocabulary, SGNS focuses on distinguishing the actual context words from a number of randomly sampled 'negative' words.

Given a pair of words (target word $w$ and context word $c$), the model learns to assign high probabilities to the actual context words and low probabilities to the negative samples. This is achieved by maximizing the log probability of the sigmoid function $\sigma$, which represents the model's estimated probability that a given pair (target-context) is valid:

\begin{equation}
    \log \sigma(\mathbf{v}_c^\top \mathbf{e}_w)
\end{equation}

Here, $\mathbf{e}_w$ is the embedding of the target word, and $\mathbf{v}_c$ is the embedding of the context word. The negative samples are then introduced in the loss function, where the model maximizes the log probability of the sigmoid of the negative of the dot product between the target word embedding and the embedding of a negative sample $c_i$:

\begin{equation}
    -\log \sigma(-\mathbf{v}_{c_i}^\top \mathbf{e}_w)
\end{equation}

The overall objective combines these terms for the positive and negative samples, summing across all positive pairs observed in the training data and a set of negative samples drawn according to a noise distribution, often related to word frequency.

By focusing only on a small subset of negative samples rather than the entire vocabulary, SGNS reduces computational complexity significantly, allowing for faster training over large datasets.

\begin{algorithm}
\caption{Word2Vec Training Algorithm}
\begin{algorithmic}[1]
\State Initialize word embeddings for the target words and context words
\For{each epoch}
    \For{each word $w$ in the corpus}
        \State context = GetContextWords($w$, window\_size)
        \For{each context word $c$ in context}
            \State // Positive sample training
            \State $z$ = DotProduct(embedding[$w$], embedding[$c$])
            \State Update embeddings using gradient ascent with the objective: $\log(\sigma(z))$
            \State // Negative sampling training
            \For{$i = 1$ to $k$} // $k$ is the number of negative samples
                \State $c_i$ = SampleNegativeWord()
                \State $z_i$ = DotProduct(embedding[$w$], embedding[$c_i$])
                \State Update embeddings using gradient ascent with the objective: $\log(\sigma(-z_i))$
            \EndFor
            \State // Update the embeddings
            \State embedding[$w$] = embedding[$w$] - learning\_rate * gradient($w$)
            \State embedding[$c$] = embedding[$c$] - learning\_rate * gradient($c$)
            \For{$i = 1$ to $k$}
                \State embedding[$c_i$] = embedding[$c_i$] - learning\_rate * gradient($c_i$)
            \EndFor
        \EndFor
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Q6.5 How would you proceed to train a part-of-speech tagger (i.e., you want to assign each word with its part of speech) if you only could use pre-trained word embeddings and MLP classifier?}

I don't know mucb about this question, so this is what GPT-4 came up with:

\subsection*{Data Preparation}
\begin{itemize}
    \item Acquire a dataset with words annotated with POS tags.
    \item Tokenize and embed words using pre-trained vectors.
    \item Construct input features from contextual embeddings.
    \item Encode POS tags with one hot vectors.
\end{itemize}

\subsection*{MLP Architecture}
\begin{itemize}
    \item Input layer: Size equal to concatenated embedding vectors.
    \item Hidden layers: One or more, with activation functions like ReLU.
    \item Output layer: Softmax with a neuron for each POS tag.
\end{itemize}

\subsection*{Training Process}
\begin{itemize}
    \item Divide data into training, validation, and test sets.
    \item Train with categorical cross-entropy and optimizers like Adam.
    \item Use validation set for hyperparameter tuning and early stopping.
\end{itemize}

\subsection*{Evaluation}
\begin{itemize}
    \item Assess performance on the test set.
    \item Use classification metrics such as accuracy and F1 score.
\end{itemize}

\subsection*{Post-Processing}
\begin{itemize}
    \item Optionally apply sequence modeling techniques like CRFs.
\end{itemize}

\part{Lecture 7}

\section{Q7.1 Describe kk-nearest neighbors prediction, both for regression and classification. Define Lp norm and describe uniform, inverse, and softmax weighting}

\subsection*{Regression}
For regression, k-NN predicts the target value by a weighted average of the targets of the k nearest neighbors:
\[ t = \frac{\sum_{i} w_i \cdot t_i}{\sum_{j} w_j} \]

\subsection*{Classification}
For classification, k-NN uses voting among the k nearest neighbors. For uniform weights:
\[ \text{class} = \text{mode}\{t_1, t_2, \ldots, t_k\} \]
With non-uniform weights, the predicted class maximizes the weighted sum of targets:
\[ \text{class} = \arg\max \sum_{i} w_i \cdot t_{i,k} \]

\subsection*{Lp-norm}
The \( L_p \)-norm is defined as:
\[ \lVert x - y \rVert_p = \left( \sum_{i=1}^{D} |x_i - y_i|^p \right)^{1/p} \]

\subsection*{Weighting Methods}
\begin{itemize}
    \item Uniform: \( w_i = 1 \)
    \item Inverse: \( w_i = \frac{1}{\text{distance}(x, x_i)} \)
    \item Softmax: \( w_i = \frac{\exp(-\text{distance}(x, x_i))}{\sum_j \exp(-\text{distance}(x, x_j))} \)
\end{itemize}

\section{L2-Regularization from Bayesian Inference}

Assuming a Gaussian prior for model parameters $\mathbf{w}$ with zero mean and variance $\sigma^2$, the prior distribution is given as $p(\mathbf{w}_i) = \mathcal{N}(\mathbf{w}_i; 0, \sigma^2)$. Consequently, the prior over all weights $\mathbf{w}$ is $p(\mathbf{w}) = \prod_{i=1}^N \mathcal{N}(\mathbf{w}_i; 0, \sigma^2) = \mathcal{N}(\mathbf{w}; 0, \sigma^2 \mathbf{I})$. The maximum a posteriori (MAP) estimation is then:

\begin{align*}
\mathbf{w}_{\text{MAP}} &= \arg \max_{\mathbf{w}} p(\mathbf{X} | \mathbf{w})p(\mathbf{w}) \\
&= \arg \max_{\mathbf{w}} \prod_{i=1}^N p(\mathbf{x}_i | \mathbf{w})p(\mathbf{w}) \\
&= \arg \min_{\mathbf{w}} \sum_{i=1}^N \left( -\log p(\mathbf{x}_i | \mathbf{w}) - \log p(\mathbf{w}) \right).
\end{align*}

Incorporating the Gaussian prior probability, we get the L2-regularized objective:

\begin{align*}
\mathbf{w}_{\text{MAP}} &= \arg \min_{\mathbf{w}} \left[ \sum_{i=1}^N -\log p(\mathbf{x}_i | \mathbf{w}) + \frac{D}{2} \log(2\pi\sigma^2) + \frac{\| \mathbf{w} \|^2}{2\sigma^2} \right],
\end{align*}

which is the L2-regularization term.


\end{document}


